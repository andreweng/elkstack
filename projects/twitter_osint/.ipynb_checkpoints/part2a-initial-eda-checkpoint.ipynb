{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Analysis using Elasticsearch and Python: Initial Discovery with Kibana\n",
    "## OSINT with Python and ELKstack [Part 2a]\n",
    "> Andrew Eng | 2020-10-09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Synched\n",
    "\n",
    "In [part 1](https://medium.com/swlh/open-source-intelligence-with-elasticsearch-analyzing-twitter-feeds-part-1-of-3-21a8b65dde03) we:\n",
    "1. Built our infrastructure using a containerized version of ELKSTACK (Elasticsearch, Logstash, Kibana)\n",
    "2. Explored and built a python script that reaches out to Twitter using APIs\n",
    "3. Transferred the collection data into Elasticsearch for analyzing\n",
    "\n",
    "In part 2, we are going to explore the data that we ingested into Elasticsearch using Kibana.\n",
    "\n",
    "**While writing this part, I realized that this is going to be a huge topic.  While going through EDA, I found so many useful tools and techniques that could be used to speed up the process.  I'm going to break part 2 into multiple sub-sections.  This sub-section is going to be focused on the initial EDA and building out simple visualizations with Kibana.  The next sections will focus on cleaning up the data using machine learning and tap into other APIs that will help us answer some questions.  There may be additional quesitons that we come up with through the process.**\n",
    "\n",
    "Since [part 1](https://medium.com/swlh/open-source-intelligence-with-elasticsearch-analyzing-twitter-feeds-part-1-of-3-21a8b65dde03), I expanded the search criteria to include all of the metadata from each tweet.  That way, I can go back to it and answer possible new questions around the data I'm exploring.  I need to clean it up a bit more.  **Note** I need to create TRY exceptions around KeyError codes (some fields do not have data, but I still want to collect that field).\n",
    "\n",
    "Since the code was getting larger than I expected.  I took out some uneeded things.  I simplified the feed to elastic function:\n",
    "\n",
    "```python\n",
    "\n",
    "while count < len(feed):\n",
    "        doc = {\n",
    "            '@timestamp': dt.now(),\n",
    "            'created_at': str(feed[count]['created_at']),\n",
    "            'twitter_id' : int(feed[count]['id']),\n",
    "            'id_str' : int(feed[count]['id_str']),\n",
    "            'full_text' : str(feed[count]['full_text']),\n",
    "            'truncated' : str(feed[count]['truncated']),\n",
    "            'display_text_range' : str(feed[count]['display_text_range']),\n",
    "            'entities' : str(feed[count]['entities']), # Already split the dictionary, no longer needed \n",
    "            'metadata' : str(feed[count]['metadata']),\n",
    "            'source' : str(feed[count]['source']),\n",
    "            'in_reply_to_status_id' : str(feed[count]['in_reply_to_status_id']),\n",
    "            'in_reply_to_status_id_str' : str(feed[count]['in_reply_to_status_id_str']),\n",
    "            'in_reply_to_user_id' : str(feed[count]['in_reply_to_user_id']),\n",
    "            'in_reply_to_user_id_str' : str(feed[count]['in_reply_to_user_id_str']),\n",
    "            'in_reply_to_screen_name' : str(feed[count]['in_reply_to_screen_name']),\n",
    "            'user' : str(feed[count]['user']), # Already split the dictionary, no longer needed\n",
    "            'geo' : str(feed[count]['geo']),\n",
    "            ... [truncated]\n",
    "```\n",
    "\n",
    "This clean up was pretty fun.  I was doing it manually, but it was taking forever.  [**VERY** tedius] I had to open the dictionary, extract the keys, format the text so it creates variables and then those variables are called.  So intead...  I turned to python.  I had to think of ways I can keep iterating through dictionaries and sub-dictionaries to extract the items for each key.  I want a simple dictionary that I can stat on.  **Note** Clean up the collection function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I moved some code blocks around and wrapped it in a function that I can use to call.  I did this because it provides better flexibility when try/exceptions are being executed.\n",
    "\n",
    "**This code block imports modules, sets server and API configurations, and objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tweepy as tw\n",
    "import tweepy as tw\n",
    "import sys\n",
    "from datetime import datetime as dt\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Initializing objects\n",
    "twitter_cred = dict()\n",
    "api = ''\n",
    "es = ''\n",
    "\n",
    "def setConfig(server):\n",
    "    # Import keys from a saved file instead of inputting it directly into the script.  \n",
    "    # Strip whitespaces and split on = as I only want the key values\n",
    "    # Server argument is the elasticsearch node\n",
    "    \n",
    "    key_location = 'twitter.keys'\n",
    "    apikeys = []\n",
    "\n",
    "    global api\n",
    "    global es\n",
    "\n",
    "    with open(key_location) as keys:\n",
    "        for i in keys:\n",
    "            apikeys.append(i.split(\"=\")[1].strip(\" \").strip(\"\\n\"))\n",
    "    keys.close()\n",
    "\n",
    "    # Initialize dictionary\n",
    "    #twitter_cred = dict()\n",
    "\n",
    "    # Enter API keys\n",
    "    twitter_cred[\"CONSUMER_KEY\"] = apikeys[0]\n",
    "    twitter_cred[\"CONSUMER_SECRET\"] = apikeys[1]\n",
    "\n",
    "    # Access Tokens\n",
    "    twitter_cred[\"ACCESS_KEY\"] = apikeys[2]\n",
    "    twitter_cred[\"ACCESS_SECRET\"] = apikeys[3]\n",
    "\n",
    "    # Set authentication object\n",
    "    auth = tw.OAuthHandler(twitter_cred[\"CONSUMER_KEY\"], twitter_cred[\"CONSUMER_SECRET\"])\n",
    "    auth.set_access_token(twitter_cred[\"ACCESS_KEY\"], twitter_cred[\"ACCESS_SECRET\"])\n",
    "\n",
    "    # Create api object with authentication\n",
    "    api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    # Set Elasticsearch Server\n",
    "    es = Elasticsearch(server, port=9200)\n",
    "\n",
    "# Execute function with the elasticsearch ip address\n",
    "setConfig('127.0.0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull a tweet so we can play around with the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = {}\n",
    "\n",
    "for tweet in tw.Cursor(api.search, q='palantir OR pltr', tweet_mode='extended').items(1):\n",
    "    feed.update(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate the dictionary and sub dictionary to generate the line(s) of code I need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at = feed['created_at']\n",
      "id = feed['id']\n",
      "id_str = feed['id_str']\n",
      "full_text = feed['full_text']\n",
      "truncated = feed['truncated']\n",
      "display_text_range = feed['display_text_range']\n",
      "source = feed['source']\n",
      "in_reply_to_status_id = feed['in_reply_to_status_id']\n",
      "in_reply_to_status_id_str = feed['in_reply_to_status_id_str']\n",
      "in_reply_to_user_id = feed['in_reply_to_user_id']\n",
      "in_reply_to_user_id_str = feed['in_reply_to_user_id_str']\n",
      "in_reply_to_screen_name = feed['in_reply_to_screen_name']\n",
      "geo = feed['geo']\n",
      "coordinates = feed['coordinates']\n",
      "place = feed['place']\n",
      "contributors = feed['contributors']\n",
      "is_quote_status = feed['is_quote_status']\n",
      "retweet_count = feed['retweet_count']\n",
      "favorite_count = feed['favorite_count']\n",
      "favorited = feed['favorited']\n",
      "retweeted = feed['retweeted']\n",
      "possibly_sensitive = feed['possibly_sensitive']\n",
      "lang = feed['lang']\n"
     ]
    }
   ],
   "source": [
    "# What keys are used\n",
    "parentList = []\n",
    "subList = []\n",
    "\n",
    "# Add dictionaries to sub process list\n",
    "def subProcess(dictionary):\n",
    "    subList.append(dictionary)\n",
    "    \n",
    "for item in feed.keys():\n",
    "    parentList.append(item)\n",
    "\n",
    "for i in parentList:\n",
    "    if type(feed[i]) is not dict:\n",
    "        print(f\"{i} = feed['{i}']\")\n",
    "        \n",
    "    else:\n",
    "        subProcess(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis with Kibana\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is the process for performing initial investigations on data.  The goal is to discover patterns, outliers, and develop hypothesis.  Visualizations can be created to help streamline this process.  \n",
    "\n",
    "> \"**[Kibana](https://www.elastic.co/kibana)** is a free and open user interface that lets you visualize your Elasticsearch data and navigate the Elastic Stack. Do anything from tracking query load to understanding the way requests flow through your apps.\"\n",
    "\n",
    "![Kibana Interface sourced from elastic.co](images/2_what_is_kibana.png)\n",
    "\n",
    "I use Kibana exclusively for EDA and creating visualizations in order to try and understand data.  As I progress, I imagine I will be using python pandas and matplotlib more to quickly sift through data.  For now, I'll stick with Kibana.\n",
    "\n",
    "[Access the notebook](https://github.com/andreweng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code snippet will not be included in the blog to reduce the read time.  This will be a supplement in my github repository\n",
    "\n",
    "import tweepy as tw\n",
    "import sys\n",
    "from datetime import datetime as dt\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Initializing objects\n",
    "twitter_cred = dict()\n",
    "api = ''\n",
    "es = ''\n",
    "\n",
    "def acqData(search, acq):\n",
    "\n",
    "    index_name = idx + dt.today().strftime('%Y-%m-%d')\n",
    "    feed = []\n",
    "    \n",
    "    print(': :Acquiring Data::')\n",
    "   \n",
    "    for tweet in tw.Cursor(api.search, q=search, tweet_mode='extended').items(acq):\n",
    "        feed.append(tweet._json)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    print(': :Transferring to Elasticsearch Search::')\n",
    "    \n",
    "    while count < len(feed):\n",
    "        doc = {\n",
    "            '@timestamp': dt.now(),\n",
    "            'created_at': str(feed[count]['created_at']),\n",
    "            'twitter_id' : int(feed[count]['id']),\n",
    "            'id_str' : int(feed[count]['id_str']),\n",
    "            'full_text' : str(feed[count]['full_text']),\n",
    "            'truncated' : str(feed[count]['truncated']),\n",
    "            'display_text_range' : str(feed[count]['display_text_range']),\n",
    "            'entities' : str(feed[count]['entities']), # Already split the dictionary, no longer needed \n",
    "            'metadata' : str(feed[count]['metadata']),\n",
    "            'source' : str(feed[count]['source']),\n",
    "            'in_reply_to_status_id' : str(feed[count]['in_reply_to_status_id']),\n",
    "            'in_reply_to_status_id_str' : str(feed[count]['in_reply_to_status_id_str']),\n",
    "            'in_reply_to_user_id' : str(feed[count]['in_reply_to_user_id']),\n",
    "            'in_reply_to_user_id_str' : str(feed[count]['in_reply_to_user_id_str']),\n",
    "            'in_reply_to_screen_name' : str(feed[count]['in_reply_to_screen_name']),\n",
    "            'user' : str(feed[count]['user']), # Already split the dictionary, no longer needed\n",
    "            'geo' : str(feed[count]['geo']),\n",
    "            'coordinates' : str(feed[count]['coordinates']),\n",
    "            'place' : str(feed[count]['place']),\n",
    "            'contributors' : str(feed[count]['contributors']),\n",
    "            #'retweeted_status' : str(feed[count]['retweeted_status']),\n",
    "            'is_quote_status' : str(feed[count]['is_quote_status']),\n",
    "            'retweet_count' : str(feed[count]['retweet_count']),\n",
    "            'favorite_count' : str(feed[count]['favorite_count']),\n",
    "            'favorited' : str(feed[count]['favorited']),\n",
    "            'retweeted' : str(feed[count]['retweeted']),\n",
    "            'lang' : str(feed[count]['lang']),\n",
    "            'user_id' : str(feed[count]['user']['id']),\n",
    "            'user_id_str' : str(feed[count]['user']['id_str']),\n",
    "            'user_name' : str(feed[count]['user']['name']),\n",
    "            'user_screen_name' : str(feed[count]['user']['screen_name']),\n",
    "            'user_location' : str(feed[count]['user']['location']),\n",
    "            'user_description' : str(feed[count]['user']['description']),\n",
    "            'user_url' : str(feed[count]['user']['url']),\n",
    "            'user_protected' : str(feed[count]['user']['protected']),\n",
    "            'user_followers_count' : str(feed[count]['user']['followers_count']),\n",
    "            'user_friends_count' : str(feed[count]['user']['friends_count']),\n",
    "            'user_listed_count' : str(feed[count]['user']['listed_count']),\n",
    "            'user_created_at' : str(feed[count]['user']['created_at']),\n",
    "            'user_favourites_count' : str(feed[count]['user']['favourites_count']),\n",
    "            'user_utc_offset' : str(feed[count]['user']['utc_offset']),\n",
    "            'user_time_zone' : str(feed[count]['user']['time_zone']),\n",
    "            'user_geo_enabled' : str(feed[count]['user']['geo_enabled']),\n",
    "            'user_verified' : str(feed[count]['user']['verified']),\n",
    "            'user_statuses_count' : str(feed[count]['user']['statuses_count']),\n",
    "            'user_lang' : str(feed[count]['user']['lang']),\n",
    "            'user_contributors_enabled' : str(feed[count]['user']['contributors_enabled']),\n",
    "            'user_is_translator' : str(feed[count]['user']['is_translator']),\n",
    "            'user_is_translation_enabled' : str(feed[count]['user']['is_translation_enabled']),\n",
    "            'user_profile_background_color' : str(feed[count]['user']['profile_background_color']),\n",
    "            'user_profile_background_image_url' : str(feed[count]['user']['profile_background_image_url']),\n",
    "            'user_profile_background_image_url_https' : str(feed[count]['user']['profile_background_image_url_https']),\n",
    "            'user_profile_background_tile' : str(feed[count]['user']['profile_background_tile']),\n",
    "            'user_profile_image_url' : str(feed[count]['user']['profile_image_url']),\n",
    "            'user_profile_image_url_https' : str(feed[count]['user']['profile_image_url_https']),\n",
    "            #'user_profile_banner_url' : str(feed[count]['user']['profile_banner_url']),\n",
    "            'user_profile_link_color' : str(feed[count]['user']['profile_link_color']),\n",
    "            'user_profile_sidebar_border_color' : str(feed[count]['user']['profile_sidebar_border_color']),\n",
    "            'user_profile_sidebar_fill_color' : str(feed[count]['user']['profile_sidebar_fill_color']),\n",
    "            'user_profile_text_color' : str(feed[count]['user']['profile_text_color']),\n",
    "            'user_profile_use_background_image' : str(feed[count]['user']['profile_use_background_image']),\n",
    "            'user_has_extended_profile' : str(feed[count]['user']['has_extended_profile']),\n",
    "            'user_default_profile' : str(feed[count]['user']['default_profile']),\n",
    "            'user_default_profile_image' : str(feed[count]['user']['default_profile_image']),\n",
    "            'user_following' : str(feed[count]['user']['following']),\n",
    "            'user_follow_request_sent' : str(feed[count]['user']['follow_request_sent']),\n",
    "            'user_notifications' : str(feed[count]['user']['notifications']),\n",
    "            'user_translator_type' : str(feed[count]['user']['translator_type']),\n",
    "            'entities_hashtags' : str(feed[count]['entities']['hashtags']),\n",
    "            'entities_symbols' : str(feed[count]['entities']['symbols']),\n",
    "            'entities_user_mentions' : str(feed[count]['entities']['user_mentions']),\n",
    "            'entities_urls' : str(feed[count]['entities']['urls']),\n",
    "            #'entities_media' : str(feed[count]['entities']['media']),\n",
    "            #'extended_entities_media' : str(feed[count]['extended_entities']['media']),\n",
    "            'metadata_iso_language_code' : str(feed[count]['metadata']['iso_language_code']),\n",
    "            'metadata_result_type' : str(feed[count]['metadata']['result_type']),\n",
    "            #'retweeted_status_created_at' : str(feed[count]['retweeted_status']['created_at']),\n",
    "            #'retweeted_status_id' : str(feed[count]['retweeted_status']['id']),\n",
    "            #'retweeted_status_id_str' : str(feed[count]['retweeted_status']['id_str']),\n",
    "            #'retweeted_status_full_text' : str(feed[count]['retweeted_status']['full_text']),\n",
    "            #'retweeted_status_truncated' : str(feed[count]['retweeted_status']['truncated']),\n",
    "            #'retweeted_status_display_text_range' : str(feed[count]['retweeted_status']['display_text_range']),\n",
    "            #'retweeted_status_source' : str(feed[count]['retweeted_status']['source']),\n",
    "            #'retweeted_status_in_reply_to_status_id' : str(feed[count]['retweeted_status']['in_reply_to_status_id']),\n",
    "            #'retweeted_status_in_reply_to_status_id_str' : str(feed[count]['retweeted_status']['in_reply_to_status_id_str']),\n",
    "            #'retweeted_status_in_reply_to_user_id' : str(feed[count]['retweeted_status']['in_reply_to_user_id']),\n",
    "            #'retweeted_status_in_reply_to_user_id_str' : str(feed[count]['retweeted_status']['in_reply_to_user_id_str']),\n",
    "            #'retweeted_status_in_reply_to_screen_name' : str(feed[count]['retweeted_status']['in_reply_to_screen_name']),\n",
    "            #'retweeted_status_geo' : str(feed[count]['retweeted_status']['geo']),\n",
    "            #'retweeted_status_coordinates' : str(feed[count]['retweeted_status']['coordinates']),\n",
    "            #'retweeted_status_place' : str(feed[count]['retweeted_status']['place']),\n",
    "            #'retweeted_status_contributors' : str(feed[count]['retweeted_status']['contributors']),\n",
    "            #'retweeted_status_is_quote_status' : str(feed[count]['retweeted_status']['is_quote_status']),\n",
    "            #'retweeted_status_retweet_count' : str(feed[count]['retweeted_status']['retweet_count']),\n",
    "            #'retweeted_status_favorite_count' : str(feed[count]['retweeted_status']['favorite_count']),\n",
    "            #'retweeted_status_favorited' : str(feed[count]['retweeted_status']['favorited']),\n",
    "            #'retweeted_status_retweeted' : str(feed[count]['retweeted_status']['retweeted']),\n",
    "            #'retweeted_status_possibly_sensitive' : str(feed[count]['retweeted_status']['possibly_sensitive']),\n",
    "            #'retweeted_status_lang' : str(feed[count]['retweeted_status']['lang']),\n",
    "            #'retweeted_status_entities_hashtags' : str(feed[count]['retweeted_status']['entities']['hashtags']),\n",
    "            #'retweeted_status_entities_symbols' : str(feed[count]['retweeted_status']['entities']['symbols']),\n",
    "            #'retweeted_status_entities_user_mentions' : str(feed[count]['retweeted_status']['entities']['user_mentions']),\n",
    "            #'retweeted_status_entities_urls' : str(feed[count]['retweeted_status']['entities']['urls']),\n",
    "            #'retweeted_status_entities_media' : str(feed[count]['retweeted_status']['entities']['media']),\n",
    "            #'user_entities_description_urls': str(feed['user']['entities']['description']['urls'])\n",
    "            'word_list':  str(feed[count]['full_text']).split(' ')\n",
    "        }\n",
    "\n",
    "        es.index(index=index_name, body=doc)\n",
    "        \n",
    "        count +=1\n",
    "    \n",
    "    print(f'Processed {tweet_count} records of {search} to {server}')\n",
    "    \n",
    "# Set credentials \n",
    "def setConfig(server):\n",
    "    # Import keys from a saved file instead of inputting it directly into the script.  \n",
    "    # Strip whitespaces and split on = as I only want the key values\n",
    "    key_location = 'twitter.keys'\n",
    "    apikeys = []\n",
    "\n",
    "    global api\n",
    "    global es\n",
    "\n",
    "    with open(key_location) as keys:\n",
    "        for i in keys:\n",
    "            apikeys.append(i.split(\"=\")[1].strip(\" \").strip(\"\\n\"))\n",
    "    keys.close()\n",
    "\n",
    "    # Initialize dictionary\n",
    "    #twitter_cred = dict()\n",
    "\n",
    "    # Enter API keys\n",
    "    twitter_cred[\"CONSUMER_KEY\"] = apikeys[0]\n",
    "    twitter_cred[\"CONSUMER_SECRET\"] = apikeys[1]\n",
    "\n",
    "    # Access Tokens\n",
    "    twitter_cred[\"ACCESS_KEY\"] = apikeys[2]\n",
    "    twitter_cred[\"ACCESS_SECRET\"] = apikeys[3]\n",
    "\n",
    "    # Set authentication object\n",
    "    auth = tw.OAuthHandler(twitter_cred[\"CONSUMER_KEY\"], twitter_cred[\"CONSUMER_SECRET\"])\n",
    "    auth.set_access_token(twitter_cred[\"ACCESS_KEY\"], twitter_cred[\"ACCESS_SECRET\"])\n",
    "\n",
    "    # Create api object with authentication\n",
    "    api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    # Set Elasticsearch Server\n",
    "    es = Elasticsearch(server, port=9200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this notebook, I the below cell is a modified version that allows me to just execute the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": :Acquiring Data::\n",
      ": :Transferring to Elasticsearch Search::\n"
     ]
    }
   ],
   "source": [
    "# You can modify this cell\n",
    "\n",
    "try:\n",
    "    idx = 'default-'\n",
    "    tweet_count = 100\n",
    "    search = 'palantir OR PLTR'\n",
    "\n",
    "    setConfig('127.0.0.1')\n",
    "    acqData(str(search), int(tweet_count))\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Kibana Index\n",
    "\n",
    "Great!  We got data into Elasticsearch.  Now, let's create a Kibana index of it:\n",
    "\n",
    "1. Browse to http://127.0.0.1:5601 > Click on \"Explore on my own\"\n",
    "\n",
    "![1_initial.png](images/kibana/1_initial.png)\n",
    "\n",
    "2. Click \"Stack management\"\n",
    "\n",
    "![2_stack_management.png](images/kibana/2_stack_management.png)\n",
    "\n",
    "3. Create Index Patterns\n",
    "\n",
    "![3_index_patterns.png](images/kibana/3_index_patterns.png)\n",
    "\n",
    "4. Create Index\n",
    "\n",
    "![4_create_index.png](images/kibana/4_create_index.png)\n",
    "\n",
    "5. Match \"default\"\n",
    "\n",
    "![5_default.png](images/kibana/5_default.png)\n",
    "\n",
    "6. Select Time Filter\n",
    "\n",
    "![6_time_filter.png](images/kibana/6_time_filter.png)\n",
    "\n",
    "7. Complete and finish up\n",
    "\n",
    "![7_complete.png](images/kibana/7_complete.png)\n",
    "\n",
    "8. Go to \"Discover\"\n",
    "\n",
    "![8_discover.png](images/kibana/8_discover.png)\n",
    "\n",
    "9. Select Time Period\n",
    "\n",
    "![9_thisWeek.png](images/kibana/9_thisWeek.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial questions we are trying to answer are:\n",
    "\n",
    "- What are the most retweets of the specific search term? (Tracking topic popularity)\n",
    "- How many unique users are tweeting about a given search term? (Is it trending?)\n",
    "- What other tweets are they posting about the search term? (Are they a troll / bot?)\n",
    "- How many likes does the person have on the search term? (Trending / Sentiments)?\n",
    "\n",
    "The following fields seems interesting when Scanning through the \"Available Fields\":\n",
    "\n",
    "- full_text\n",
    "- lang\n",
    "- retweet_count\n",
    "- user_created_at\n",
    "- user_screen_name\n",
    "- user_followers_count\n",
    "- user_friends_count\n",
    "- user_statuses_count\n",
    "- user_created_at\n",
    "\n",
    "I added lang field in so I can later figure how what is the sentiments per \"language\" ratio.  For now, I'm filtering out non-english.\n",
    "\n",
    "![1_parsing_english](images/eda_kibana/1_parse_english.png)\n",
    "\n",
    "Filtering english-only reduces my sample from 400 to 262.\n",
    "\n",
    "Scanning through the full_text, I see there are a bunch of retweets that I want to aggregate into 1.  The stats show that the top 5 values are retweets and accounted for X percentage of the 262 tweets.  We see that the top retweet accounted for 19.1%.  Let's explore that.\n",
    "\n",
    "![2_full_text_stats.png](images/eda_kibana/2_full_text_stats.png)\n",
    "\n",
    "I got super excited as I thought it was that easy and found all the bots.  However, a closer look.  retweet_count is NOT the amount of times the user_screen_name retweeted.  It's how many times the full_text was retweeted.\n",
    "\n",
    "![3_retweet_count.png](images/eda_kibana/3_retweet_count.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2_kibana_grabbing_relevent_info.png](images/2_kibana_grabbing_relevent_info.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the most retweets of the specific search term? (Tracking topic popularity)\n",
    "Scanning through the defaults fields list we see:\n",
    "    \n",
    "![retweet](images/eda_kibana/retweet.png)\n",
    "\n",
    "Now, let's create a visualization:\n",
    "\n",
    "Click on \"Visualize\" > \"Create Visualization\" > select \"Data Table\" > select the index to look in \"default*\" > Click \"Add Bucket\" > \"Split rows\" > Select Aggregation > Terms:\n",
    "- Field: full_text.keyword\n",
    "- Change size from 5 to 25\n",
    "\n",
    "Click \"> Update\"\n",
    "\n",
    "![buckets.png](images/eda_kibana/4_buckets.png)\n",
    "\n",
    "**The answer to the question is: \"RT @_whitneywebb: NEW ARTICLE: A secretive AI platform powered by Palantir will soon be fed data from a new national \"smart sewer\" networâ€¦\" with a count of 50 retweets during the time of collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique users are tweeting about a given search term? (Is it trending?)\n",
    "We're going to dive right into this with a pie graph and a counter:\n",
    "\n",
    "Visualize > Create New Visualization > Metric > Default* > Metrics > Aggregation: Unique Count > Field: user_screen_name.keyword > Save the Visualization\n",
    "\n",
    "Visualize > Create new Visualization > Pie > Buckets: Split Slices > Aggregation: Terms > Field: user_screen_name.keyword > size 10\n",
    "\n",
    "Visualize > Create new Visualization > Line > Metrics (Y-axis): Count\n",
    "Buckets (X-axis): Aggregation: Terms > Field: user_screen_name.keyword > size 10\n",
    "\n",
    "Now that we have the visualizations, we can create a dashboard:\n",
    "\n",
    "Create new dashboard:\n",
    "- Add all the visualizations\n",
    "\n",
    "Search for \"A secretive AI platform powered by Palantir will soon be fed data from a new national\"\n",
    "\n",
    "![dashboard](images/eda_kibana/dashboard.png)\n",
    "\n",
    "**The answer is 14 unique users retweeting the same thing during this time frame.  They tweeted the same amount of times (4), maybe they are bots?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What other tweets are they posting about the search term? (Are they a troll / bot?)\n",
    "Since we have the visualizations and dashboard setup already, we can just modify the search criteria to:\n",
    "\n",
    "```text\n",
    "\"purofierro666\" OR \"pennys_shevy\" OR \"nevrsurrendr\" OR \"eustache_luigy\" OR \"eric_davao\" OR \"Inanna432\" OR \"LisaMP925\" OR \"SultryRobin\" OR \"UsernameNAB\" OR \"WarmongerExpose\"\n",
    "```\n",
    "\n",
    "![img_user_count](images/eda_kibana/users_count.png)\n",
    "\n",
    "It's all the same retweet.  But there's a few things that we have to keep in mind:\n",
    "1. Our collection script is only acquiring the search keywords and nothing else, if the users didn't tweet about the search keywords, we wouldn't pick it up.  \n",
    "2. My search timeframe is pretty narrow, I collected only 100 tweets few minutes apart.\n",
    "3. I have to play around with the #tweet count.  Users reportedly tweeted 4 times, but I can't seem to find it.\n",
    "\n",
    "**I would still list this as unanswered because of the limitation of our collection script and perhaps how we are processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many likes does the person have on the search term? (Trending / Sentiments)?\n",
    "Looking at the data, it looks like I have to adjust the script to take in additional metadata.  This question will remain unanswered.  This may not even be relevent.  I added this question as a technique to detecting potential bots and perform chain link analysis of the bot network.  But I don't know if this will work out the way I wanted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting Thoughts\n",
    "\n",
    "Out of the initial questions that I had, I was able to definitively answer 2 of 4 for my time period.  I discovered that I am missing some fields that I may need and that some questions may not be relevent.  Once I build out the visualizations and the dashboard, I can modify the search parameter and all visualizations will update.  Kibana is nice to visualize data fast and efficiently.\n",
    "\n",
    "part2b will be going back to the script to see what we can modify and adjust to take in additional information and to enrich the records further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
